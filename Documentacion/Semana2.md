<h1 align="center">  Semana 2: Data Engineering </h1>

##  Indice
1. [Descripcion de la semana](#descripcion)
2. [Diagrama E-R](#e-r)
3. [Diccionario](#dicc)
4. [Data Warehouse y Pipeline](#dw)
5. [Flujo de Trabajo](#workflow)

## 1. Descripci贸n de la semana. <a name="descripcion"></a>

El enfoque de la labor llevada a cabo durante la segunda semana del proyecto se centr贸 en el 谩mbito de la ingenier铆a de datos. Este proceso abarc贸 la extracci贸n de datos desde diversas fuentes, la posterior depuraci贸n para asegurar su calidad y, finalmente, la carga de estos datos en la infraestructura del Data Warehouse seleccionado. Para llevar a cabo estas tareas, se distribuyeron responsabilidades entre los miembros del equipo de trabajo, y los hitos y plazos de ejecuci贸n se encuentran representados en el Diagrama de Gantt adjunto. Este diagrama proporciona una visi贸n detallada de las actividades realizadas y su programaci贸n.


## 2. Data Warehouse y Diagrama E-R <a name="e-r"></a>
Cuando implementamos la estructura de almacenamiento, optamos por utilizar un Data Warehouse a trav茅s de la herramienta BigQuery, que forma parte de la plataforma de Google Cloud (GCP). Debido a que se trata de un modelo de datos relacionales, elaboramos un diagrama de Entidad-Relaci贸n que visualiza de manera gr谩fica la disposici贸n l贸gica de nuestra base de datos.

<img src="src/modelo_er.drawio(1).png" alt="modelo ER">

## 3. Diccionario de datos <a name="dicc"></a>

Con base en el diagrama E-R mencionado previamente, hemos creado un diccionario de datos para aclarar la interpretaci贸n de la informaci贸n y verificar su precisi贸n. Puedes consultarlo en el siguiente.


## 4. Pipeline <a name="dw"></a>

El siguiente diagrama proporciona un desglose detallado del pipeline que ilustra claramente el proceso de ETL desarrollado anteriormente. Por un lado, se destacan varias fuentes de datos, que contin煤an hacia el proceso de limpieza, transformaci贸n y validaci贸n realizado en Python con la librer铆a Pandas, para posteriormente ser cargados en la base de datos de Google BigQuery. Este flujo hacia el Data Warehouse se conoce como carga inicial. En cuanto a la carga incremental, que se ejecuta de manera autom谩tica cada mes, se emplean herramientas proporcionadas por GCP: Cloud Scheduler, Cloud Pub/Sub, Cloud Function y Cloud Storage. Despu茅s de este proceso, los datos se cargan en la base de datos relacional de BigQuery.

<img src="src/ETL.drawio.png" alt="pipelines">

## 5. Flujo de Trabajo <a name="workflow"></a>

En lo que respecta al proceso de trabajo, el diagrama que sigue ofrece una vista integral del ciclo completo de vida de los datos en el proyecto. Se inicia con la recopilaci贸n de datos, seguida por su integraci贸n y carga en el Data Warehouse. Finalmente, se incluyen los distintos procesos que acceder谩n a la base de datos: en un extremo se halla el Modelo de Machine Learning, que emplea Scikit-Learn como su herramienta, mientras que en el otro se encuentra la generaci贸n de informes y visualizaciones, con el objetivo de crear un panel de control utilizando PowerBI.

<img src="flujo_del_dato.drawio.png" alt="flujo del dato">
--------------------------------------------------------------------------------------------------------------------------------------------------------------
## Semana 2: An谩lisis Profundo de los Datos e Informes

Durante la segunda semana del proyecto, nuestro enfoque principal fue llevar a cabo un an谩lisis detallado de los datos recopilados, con el objetivo de obtener una comprensi贸n completa del panorama actual del mercado de taxis en la ciudad de Nueva York.

### Actividades Realizadas:

1. **An谩lisis Detallado de los Datos:**
   - Realizamos un an谩lisis exhaustivo de los datasets proporcionados, explorando cada variable y comprendiendo la estructura de los datos.

2. **ETL en Servicio Cloud (GCP) para BigQuery:**
   - Implementamos un proceso ETL (Extract, Transform, Load) en un servicio cloud, en este caso Google Cloud Platform (GCP), para organizar y almacenar los datos en BigQuery, facilitando su posterior manipulaci贸n y an谩lisis.

3. **Correcci贸n y Refinamiento de KPIs:**
   - Basados en el an谩lisis de los datos, revisamos y refinamos los KPIs previamente establecidos para asegurarnos de que estuvieran alineados con la informaci贸n disponible y proporcionaran m茅tricas precisas.

4. **Desarrollo de un Minimum Viable Product (MVP) de Dashboard:**
   - Creamos un prototipo de tablero de control que presentaba de manera visual algunos de los insights clave derivados del an谩lisis de datos. Este MVP proporcion贸 una idea inicial de c贸mo ser铆a el producto final.

### Resultados y Conclusiones:

Al finalizar la semana 2, hab铆amos logrado un profundo entendimiento de los datos y hab铆amos desarrollado un sistema eficiente para su gesti贸n en la nube. Tambi茅n revisamos y ajustamos los KPIs para asegurarnos de que estuvieran bien alineados con la informaci贸n disponible. El MVP del dashboard nos permiti贸 visualizar de manera preliminar los insights m谩s relevantes, lo que nos proporcion贸 una visi贸n clara del valor que este proyecto puede aportar a la empresa interesada.
