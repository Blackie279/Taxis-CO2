<h1 align="center">  Semana 2: Data Engineering </h1>

##  Indice
1. [Descripcion de la semana](#descripcion)
2. [Diagrama E-R](#e-r)
3. [Diccionario](#dicc)
4. [Data Warehouse y Pipeline](#dw)
5. [Flujo de Trabajo](#workflow)

## 1. Descripci贸n de la semana. <a name="descripcion"></a>

El enfoque de la labor llevada a cabo durante la segunda semana del proyecto se centr贸 en el 谩mbito de la ingenier铆a de datos. Este proceso abarc贸 la extracci贸n de datos desde diversas fuentes, la posterior depuraci贸n para asegurar su calidad y, finalmente, la carga de estos datos en la infraestructura del Data Warehouse seleccionado. Para llevar a cabo estas tareas, se distribuyeron responsabilidades entre los miembros del equipo de trabajo, y los hitos y plazos de ejecuci贸n se encuentran representados en el Diagrama de Gantt adjunto. Este diagrama proporciona una visi贸n detallada de las actividades realizadas y su programaci贸n.


## 2. Data Warehouse y Diagrama E-R <a name="e-r"></a>
Cuando implementamos la estructura de almacenamiento, optamos por utilizar un Data Warehouse a trav茅s de la herramienta BigQuery, que forma parte de la plataforma de Google Cloud (GCP). Debido a que se trata de un modelo de datos relacionales, elaboramos un diagrama de Entidad-Relaci贸n que visualiza de manera gr谩fica la disposici贸n l贸gica de nuestra base de datos.


![ETL](https://github.com/Blackie279/Taxis-CO2/blob/main/src/modeloER.drawio.jpg)

## 3. Diccionario de datos <a name="dicc"></a>

Con base en el diagrama E-R mencionado previamente, hemos creado un diccionario de datos para aclarar la interpretaci贸n de la informaci贸n y verificar su precisi贸n. Puedes consultarlo en el siguiente.


## 4. Pipeline <a name="dw"></a>

El siguiente diagrama proporciona un desglose detallado del pipeline que ilustra claramente el proceso de ETL desarrollado anteriormente. Por un lado, se destacan varias fuentes de datos, que contin煤an hacia el proceso de limpieza, transformaci贸n y validaci贸n realizado en Python con la librer铆a Pandas, para posteriormente ser cargados en la base de datos de Google BigQuery. Este flujo hacia el Data Warehouse se conoce como carga inicial. En cuanto a la carga incremental, que se ejecuta de manera autom谩tica cada mes, se emplean herramientas proporcionadas por GCP: Cloud Scheduler, Cloud Pub/Sub, Cloud Function y Cloud Storage. Despu茅s de este proceso, los datos se cargan en la base de datos relacional de BigQuery.

![ETL](https://github.com/Blackie279/Taxis-CO2/blob/main/src/ETL.drawio.jpg)

## 5. Flujo de Trabajo <a name="workflow"></a>

En lo que respecta al proceso de trabajo, el diagrama que sigue ofrece una vista integral del ciclo completo de vida de los datos en el proyecto. Se inicia con la recopilaci贸n de datos, seguida por su integraci贸n y carga en el Data Warehouse. Finalmente, se incluyen los distintos procesos que acceder谩n a la base de datos: en un extremo se halla el Modelo de Machine Learning, mientras que en el otro se encuentra la generaci贸n de informes y visualizaciones, con el objetivo de crear un panel de control utilizando PowerBI.

![link](https://github.com/Blackie279/Taxis-CO2/blob/main/src/flujo_del_dato.drawio.jpg)

## 6. Demostracion de carga incremental automatizada
[video](https://drive.google.com/drive/folders/1SScuaGZbWpoKabwg9Mp-PzlWGMCdwqzF)

--------------------------------------------------------------------------------------------------------------------------------------------------------------

## **Desarrollo de un Minimum Viable Product (MVP) de Dashboard:**
   - Creamos un prototipo de tablero de control que presentaba de manera visual algunos de los insights clave derivados del an谩lisis de datos. Este MVP proporcion贸 una idea inicial de c贸mo ser铆a el producto final.


